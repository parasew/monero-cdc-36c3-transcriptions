# Cheng Wang

_**Critical Decentralisation Cluster 36c3 - Blockchain Sharding (Cheng Wang)**_

[youtu.be/cQKL0z1mJWw](https://youtu.be/cQKL0z1mJWw)

_**Abstract**_

In this talk, I am gonna first give a short review of the current popular sharding methods, with a focus on the open problems. Then I am gonna talk about Alephium's innovative sharding solution, and why it could the promising approach.

_**Transcription**_

_Diego:_ And we are gonna do this whole blockchain sharding talk. I probably butchered your name, can you say it for me?

_Cheng:_ Cheng Wang.

_Diego:_ I didn't butcher the name, I just probably can't hear how I butchered the name, is actually the way to say it. Ok, so we are gonna be talking about blockchain sharding here. So if you're out there, and you're interested in blockchain sharding, what it is, is it a buzz word, is it more than a buzz word, it's Cheng buzz word — all the answers to all these questions is going to be revealed. To go ahead and come on and take a seat, I'm gonna pass this off, so let’s give Cheng a big hand.

_Cheng:_ Thank you for the introduction. Yeah, so first a little bit about my background. My academic background is from number theory and the supercomputing. And then I joined the crypto space. My focus right now is sharding project, is called "Alephium". We are changed deliver decentralized and lightweight scalable blockchain with very fast course of the transactions. So in this talk I'm gonna first give a short overview about the existing like research has been done by other projects, for example Ethereum 2.0, and also some other similar projects, with highlights on the problems. And then I'm going to show you how our project is changing soffits resolving all the open problems in the field.

So here's a single blockchain that is Bitcoin or Ethereum, and we want to improve the transactions throughput. Probably all of you have heard about the scalability dilemma, it says that if we want to scale the blockchain we need to sacrifice one of the three dimensions to some extent. So actually if we look it a bit deeper, there are much more things we need to, or much more dimensions we need to consider in the tradeoff for blockchain.

So here is a more academic model to analysis blockchain. So we usually say blockchain as state machine replication system. For state machine replication there is three paths for data execution and consensus. So data part has quite a lot of constraints, we need like some assumptions about network bandwidth, and also network topology is important: is it p2p or is a little bit more centralized with different type of nodes. And then is about the execution. So in blockchain the execution first you have the state the tree, in Bitcoin it is the UTXO states, in Ethereum it is the Merkle tree. And so here there are some limitations of bottlenecks, the first is IO bottleneck, then the second part is executing model, assumed as the other commands I executed sequentially or you want to support a parallelized execution commands. The third is about the consensus, because it is we need to model, to first give assumption about networks: some of the projects assume is a synchronized network, some other projects assume is the partially  synchronized, even single ready(?). And also security model — we assume like majority on is money or majority on is the hash power on a shard. So the last most important part is for consensus we have the liveness and safety tradeoff. We cannot get both, we have to do a tradeoff here. So actually scaling blockchain is a very complicated topic, we have seen a lot of different proposals. Usually when they say: "We can achieve like one million TPS", they did not tell you like some of the dimensions actually or some of the assumptions actually sacrificed a lot.

So here is a some methods to scaling blockchain. We first have the centralized exchange. So it definitely could improve the transactions throughput a lot, but the security model is sacrificed right. And also large blocks. Large blocks could help a little bit for example to scale Bitcoin, but the sacrifice is like the network assumption, the network bandwidth and also network you wanted to assume it to be a synchronized or asynchronous or it will affect the delay, and also there are some misses I will not go through it. But sharding is still considered to be the one of the most promising way to scale blockchain. Why it is like this? Because sharding is the way to scale blockchain by parallelizing data, execution and consensus in the same time. The cyphertext(?) is we want to achieve this by very few sacrifice, sacrificing very few the other stuff for example network assumption, for example the security model, we want to keep it the same. If we want to scale blockchain, we want to make it still decentralized, we want to make assume that is still correct with honest unlike majority artist(?) hash power, something like this.

So sharding has been researched and investigated for quite a few years. As you we have all made open like almost all people in the field must have heard it and also notice that the progress is not very well, especially in Ethereum 2.0 got delay the quite a lot of years. So here I'm going to give a short review of the popular methods that is behind Ethereum 2.0 and also some other similar projects, and discuss about the progress.

So here is a very simplified like picture to demonstrate the idea of sharding. The basically is like in Bitcoin, Ethereum we have just one blockchain, and here we have a set of a number of blockchains. And the miners they don't focus on one chain but instead they focus on all of the chains. The problem here is the miners are distributed to different shards, than the security is decreased for each shard.

So the idea is to solve, the popular method to solve this is using PoS and validators sampling. The basic idea is like we have a Beacon Chain, we maintain a set of validators, and then we use a randomized algorithm to distribute these validators to different shards. So here comes the first assumption of this approach. You see, the something is basically like this we have a large set of validators, and we assumed as the validate, the like make like a very high grade of the nodes actually are honest. It's not just majority assumption anymore. Actually if you want to assume that it's pretty safe for each shard, you need to assume that around like 70 percentage or even 80 percents of the validators are honest. Of course it depends on the size of the validators. Another thing is if the size is not enough actually is pretty insecure, if you like you in the beginning, when you start a project, if you only have like 200 the validators actually you can basically see like it can never be secure. So I will not to show you the proofs but basically this is one of the assumption, and actually this assumption cannot hold in short term, because staking service will become a thing, a lot of users they don't want to run all by themselves, they will dedicate this staking to a staking service, and this will make the assumption also questionable.

This is another critical security issue with this is something called ‘adaptive adversary’. It means when you distribute validators randomly in two different shards, actually once the validators they are sent to a shard, then you can try to bribe these validators in this shard to make attack. This is very doable actually. If you give incentives to the validators by using for example a smart contract, you can automate this process. It's very like in practice it's very doable, here is a very simple example, you have a hundred shards and the 50% of tokens staked actually for each other, you only need to have 0.5 percent just take into attack it. So this is also a problem.

Another problem is about shuffling, because as we say that we need to use the validators to randomly to the nodes, and after able, because of the security issues we need to reshuffle it to make sure that the validators are not in the same shard. So the problem is shuffling actually has quite some overhead, because when you switch from one shard to another you need to sync the states of the new shard right. So this means you need to download this, if you only download the latest state of Bitcoin you need to download like several gigabits. This will take quite a long time giving depends on the network assumption. So here the problem is if we try to shuffle it too frequently, then the overhead is too high right. If we shuffle it like not very frequently, then the security is a issue, because the validators are kind of static for like half a day or for one day, then you can basically if you want attack it, you can take some action in just one day right.

There are some ways to easy this problem, one of the solution is to use stateless clients. Stateless clients is good way to solve this, but the problem is stateless clients is not very practical for the moment. For Bitcoin they are kind of a lot of researches, but still is not practical, and for years it will be even more tricky, because the small contractor is it's more complicated. Then now Ethereum is doing some research using local chip(?) proofs for this, but they are quite a bit overheard, because the local chip(?) actually the depth is pretty high. So whether we can have practical stateless clients in the near future is also open problem.

So let's assume on the previous run the validators sampling is acceptable. Then let's say we want to do transaction between the shards. How can we do it? So in this model, in the existing approach people usually use the so-called two phase commit protocol. Basically is like Alice in shard A, first if she want to transfer money to Bob in shard B, it first need to create a receipt. So here she first create a transaction to say that: "I want to make a debit this amount of token, this or this amount of money, and I want to send it to Bob in shard B". And once these transaction is got committed into shard A, then in shard B Bob could use a proof of the receipt to claim the money. So in this way it has to at least two steps or two transactions used, but in practice actually is even more complicated, because gas fee stuff and also security issues. Here I only talk about one of the issues is about the dependencies of the transactions, because right now in shard B the transaction of Bob actually depends on the transaction of Alice in shard A. So this will make if because each shard actually could have forks. So then the forks will have complicate dependencies, because of corrosion(?) transactions, and we need to take a good take care of this. If we have like a thousand and twenty four shards, the dependency graph could be like very chaotic, and it will be very hard to read off the correct dependencies.

There are some ways to solve this, but not perfect. The first way is we try to arrange the shard in a directed acyclic graph. For example in a chain(?), then whenever you receive something you are you don't only check the data of your shard you also check the data in your neighborhood shard. In this way can validate actually the course of transaction both sides is correct. So this method is the first is proposed by Vlad, but it has some issues as well, because if you have, let's say if you have a transaction from A to B and to C, then basically C cannot validate the original transaction from A, because A is not a neighborhood of C anymore yeah. So here is it kind of solve some of problem but it does not solve it completely. And the second is like you just don't handle it, depends on the finality of algorithm and some of the system, can like claim that they can never have forks, but PoS this personally I'm a bit skeptical about it.

Beside this we still have two very critical issues with sharding. So the first issue, both of the issues are also related with the dependencies. The first issue is state validity. So it means if something bad happened in your one shard, is very hard for the validators in another shard to notice this. There are some ways to solve this problem. The first one is called a fishman by pocktot(?). So basically is like you have another can type of nodes, it try to monitor the network to check if the there's something we had happened in the state, and then once they found it, they try to punish it. But this has some issues, because you will introduce some delays into it, and also still they are small some timing window you can take advantage of. The perfect way to solve this is using zk proofs to prove that something actually happening in a shard. Is a bit like that was before stateless clients. But zkp as I said before zkp for state validity is also not practical.

Another problem is data availability. So in this architecture of Ethereum 2.0 or other similar sharding misses. They always have a main chain. So the main chain it will not denote the other blocks from the other shards. It will all only download the blockheader of the shard. So in this case the validators actually they can collude for a shard and produce one block, but they only disclose, the only disclose the blockheader to the Beacon Chain. So Beacon Chain we all committed. But actually the validators they never disclose the full block. This will make the block not available, it basically best can make the system hards(?).

So we have seen quite a lot of open problems in the current approach. I call this project as PoS plus validators sampling method. Some people call it as commit based protocol or approach. So one is the issue of this approach if we want to say it describe it in one sentence.

So basically is transferring from the "don't trust, verify" approach to the "don't verify, trust" approach. So in Bitcoin or Ethereum we do in all the blocks, and we do the validation, and if something weird then we don't just reject the block. But in this new PoS plus validate sampling approach is not like this anymore, you have to chance to the validators, you have to trust also the assumption that in each shard actually no more than one-third of the right this malicious. And then you trust what they verify for you. So in the end each shard actually is a lighter node or is trustless node. But we actually even know today I don't think like we have practical mature enough light node that you can have very high security like for nodes. So I think yeah this I'm not saying that this approach is totally wrong, but is still like in the research progress, and many open problems need to be solved.

So now I would like to show you how we are going to do this, to do sharding in the field. So we proposed a new algorithm is called a BlockFlow. That's our sharding approach. So basically there are several points I want to highlight first. So first we still for in the don't trust, verify approach. This is I think, this is a critical and important to keep a high security. The thing is we use PoLW and the UTXO model. Actually this gives us a very lightweight design. I will explain it later. The set point is in our system the crucial transaction just need one step. As I said as in the existing research proposal is always two-step like commit protocol in it will affect the user experience. We have a complete solution for sharding. Is not like research in progress, we are developing this for quite a long time, and we have an alpha system, it's kind of we have a proof of concept already.

Now let's show you how we do it. So first we use UTXO model, we firstly divide addresses is randomly into G groups. In this way the transactions, because transaction could happen between any two of the groups right, so there are G x G possible types of groups of transactions. For example in the left graph we have nine types of transactions: from A to B, A to C, A to A etc. We build a blockchain for each one of these types. So in total we have G x G chains right.

So what's the beautiful thing about this kind of decomposition, so the first cool thing is if you want to do a transaction from Group B – Group A we just prepare the transaction, just like what do we do for Bitcoin, we commit to the chain corresponding to the A from B right, then it's done, we don't need to have two steps. The second thing is about so now for Group B, it does not need to turn out the transactions between A to C and C to A right, because that's not relevant for the group B. And so in this way the amount of data a single node need to save is reduced to around 1/ G.

But actually the main challenge is the, you know the data dependencies between shards, and with this is very hard to reach consensus. You can consider as like a forking system. So we proposed a very specific algorithm called BlockFlow to resolve the forks and to reach consensus. I'm not going to go to details. Here's a very conceptual description. So basically is every new block selects very special hashes as the dependencies following very specific rules. So that guarantee, that the first thing is guaranteed as there's no kind of invalid dependencies, because if one transaction depends on to forks of another shard this is invalid. So we basically should like for being this kind of dependencies. And then the second thing is like we should make sure that this kind of dependency selection is efficient enough, because otherwise we can simply choose the latest hash of all other shards. But this is not efficient and it also has other drawbacks. So once we have this specific dependencies data structure, each new block actually could be determine a unique flow of blocks, is like a net as you can see in the left graph is like a network of flow of blocks right. So each new block using these dependencies to depend on a unique flow of blocks. And then we simply choose the best flow based on the PoW proof of work weight. So now for the nodes they need to validate the dependencies and also transactions. The dependencies validation is something new can be do to Bitcoin or Ethereum.

So what is the cost. The cost is so first we need to take time to compute, use some algorithm to compute the dependencies data structure. The calculation could be very fast heuristic in most cases. And the validation is quick, is much faster than calculation even I remember is 2 seconds for G equals 32, anyway it's very well too many seconds, anyway is very fast. And also we need to have more space to stop these dependencies. Basically it's several hashes, and actually we can move this to the block body and just include them in the Micucci(?). And then for the client is not a big deal, and for the full node these dependencies is quite negligible.

So in case that's I if you are not familiar, very familiar with the technical details of sharding, so here I want to convince you like conceptually, why this approach could work. So the first way to look at it is this method is actually a shift from state machine replication to state cluster replication. State machine replication is just a single blockchain and here is like we first decompose the ledger to be stored in a small cluster. And then we replicate this cluster. And the size of the cluster we will increase from very small to large gradually. This makes total sense, because when the market cap of the blockchain is very small, actually we don't need to shard it quite a lot right. And then late when the alt option(?) increases, and the market cap increases, and the hash rate increases, then we can increase the size of the cluster. And actually in our original architecture is very easy to increase the size of the cluster. It's not that hard, it could be very dynamic.

And the last point is because is a replication, so it is still follows that don't trust, but verify approach. So we don't have all the problems from the PoS plus validators sampling method. We don't need to trust to validate this we only trust ourselves. And also in some cases you can even, but this is optional, you can even chance your friends, you can connect to your friends to form a fork ledger. I think this is a much more like acceptable than you just the random validators. I mean but this is optional, you can still run a very small clusters, because in the early days when market cap is very very low, the cluster is like you can even hold it with one node if your computer is powerful enough, or you can decompose it to run like several lights very very cheap node in Amazon is fair.

Another way to look at this approach is we can analogy this to the computing model. We all know that the computing model you have from sequential computing to concurrent building basically modulating, and then to distributed computing — you have a set of computers, each computers could do concurrence work, concurrent computing. So in blockchain actually we have the single blockchain, Bitcoin, Ethereum, and then we have some proposed DAG based blockchain. But actually most of the tech projects are not designed properly. and BlockFlow is you can see is a distributed DAG blockchain, so it's very much like the distributed computing evolution. And this is also give us you know we kind of learn from history, give us confidence. That's why it makes sense yeah.

That's all and if you are interested in more about details you can go to visit our website or follow me in Twitter, asking me questions later. And tomorrow I have another talk about how to reduce the energy consumption of proof of work. So if you are interested in please come to have a look and ask me questions.

_Diego:_ Thank you very much, Cheng. We don't have time for questions, but he, will you be able to take questions off stage over here, yeah, so he's gonna be over here if you would like to ask him some questions. We're gonna get set up for our next talk which is Monero-Bitcoin atomic swaps. That's a big buzzword everybody likes to say ‘atomic swaps’. So we're gonna get that set up working be back here in a couple of minutes. Yeah stuff is going down. Huh I'm pretty interested now, and I mean there can't be C3 without at least one something around it going down. So we'll be back very soon.
